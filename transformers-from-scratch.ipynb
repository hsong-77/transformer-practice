{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnZrMwds48HB2HMfXtfGK8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsong-77/transformer-practice/blob/main/transformers-from-scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZQezGjoyqPB"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.11.3\n",
        "!pip install datasets\n",
        "!pip install psutil\n",
        "!pip install accelerate==0.5.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "\n",
        "generation_gpt = pipeline(\"text-generation\", model=\"openai-gpt\")\n",
        "generation_gpt2 = pipeline(\"text-generation\", model=\"gpt2\")"
      ],
      "metadata": {
        "id": "v838Mvlu5GI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_size(model):\n",
        "  return sum(t.numel() for t in model.parameters())\n",
        "\n",
        "print(f\"GPT  size: {model_size(generation_gpt.model)/1000**2:.1f}M parameters\")\n",
        "print(f\"GPT2 size: {model_size(generation_gpt2.model)/1000**2:.1f}M parameters\")"
      ],
      "metadata": {
        "id": "lian-J5H75hB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def enum_pipeline_outputs(pipe, prompt, num_return_sequences):\n",
        "  out = pipe(prompt, num_return_sequences=num_return_sequences, clean_up_tokenization_spaces=True)\n",
        "  return \"\\n\".join(f\"{i+1}.\" + s[\"generated_text\"] for i, s in enumerate(out))\n",
        "\n",
        "prompt = \"\\nWhen they came back\"\n",
        "print(\"GPT completions:\\n\" + enum_pipeline_outputs(generation_gpt, prompt, 3))\n",
        "print(\"\")\n",
        "print(\"GPT-2 completions:\\n\" + enum_pipeline_outputs(generation_gpt2, prompt, 3))"
      ],
      "metadata": {
        "id": "AU1pBhZ58yqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "byte_to_unicode_map = bytes_to_unicode()\n",
        "unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\n",
        "base_vocab = list(unicode_to_byte_map.keys())\n",
        "\n",
        "length = 100000\n",
        "dataset = load_dataset(\"transformersbook/codeparrot-train\", split=\"train\", streaming=True)\n",
        "iter_dataset = iter(dataset)\n",
        "\n",
        "def batch_iterator(batch_size=10):\n",
        "  for _ in tqdm(range(0, length, batch_size)):\n",
        "    yield [next(iter_dataset)[\"content\"] for _ in range(batch_size)]\n",
        "\n",
        "new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=12500, initial_alphabet=base_vocab)"
      ],
      "metadata": {
        "id": "tlsrVvYq5wXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"transformersbook/codeparrot-small-vocabulary\")\n",
        "config = AutoConfig.from_pretrained(\"gpt2\", vocal_size=len(tokenizer))\n",
        "model = AutoModelForCausalLM.from_config(config)"
      ],
      "metadata": {
        "id": "lxdBLXSr_21e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'GPT-2 size: {model_size(model)/1000**2:.1f}M parameters')"
      ],
      "metadata": {
        "id": "Nc_s_mYqAnNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples, total_characters, total_tokens = 500, 0, 0\n",
        "dataset = load_dataset(\"transformersbook/codeparrot-train\", split=\"train\", streaming=True)\n",
        "\n",
        "for _, example in tqdm(zip(range(examples), iter(dataset)), total=examples):\n",
        "  total_characters += len(example[\"content\"])\n",
        "  total_tokens += len(tokenizer(example[\"content\"]).tokens())\n",
        "\n",
        "characters_per_token = total_characters / total_tokens\n",
        "characters_per_token"
      ],
      "metadata": {
        "id": "kvYkT2kjDyD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import IterableDataset\n",
        "\n",
        "class ConstantLengthDataset(IterableDataset):\n",
        "  def __init__(self, tokenizer, dataset, seq_length=1024, num_of_sequences=1024, chars_per_token=3.4):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.concat_token_id = tokenizer.eos_token_id\n",
        "    self.dataset = dataset\n",
        "    self.seq_length = seq_length\n",
        "    self.input_characters = seq_length * chars_per_token * num_of_sequences\n",
        "\n",
        "  def __iter__(self):\n",
        "    iterator = iter(self.dataset)\n",
        "    more_examples = True\n",
        "    while more_examples:\n",
        "      buffer, buffer_len = [], 0\n",
        "      while True:\n",
        "        if buffer_len >= self.input_characters:\n",
        "          print(f\"Buffer full: {buffer_len}>={self.input_characters:.0f}\")\n",
        "          break\n",
        "        try:\n",
        "          print(f\"Fill buffer: {buffer_len}<{self.input_characters:.0f}\")\n",
        "          buffer.append(next(iterator)[\"content\"])\n",
        "          buffer_len += len(buffer[-1])\n",
        "        except StopIteration:\n",
        "          iterator = iter(self.dataset)\n",
        "\n",
        "      all_token_ids = []\n",
        "      tokenized_inputs = self.tokenizer(buffer, truncation=False)\n",
        "      for tokenized_input in tokenized_inputs[\"input_ids\"]:\n",
        "        all_token_ids.extend(tokenized_input + [self.concat_token_id])\n",
        "\n",
        "      for i in range(0, len(all_token_ids), self.seq_length):\n",
        "        input_ids = all_token_ids[i:i+self.seq_length]\n",
        "        if len(input_ids) == self.seq_length:\n",
        "          yield torch.tensor(input_ids)"
      ],
      "metadata": {
        "id": "7UkC9DD-Vzu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_dataset = dataset.shuffle(buffer_size=100)\n",
        "constant_length_dataset = ConstantLengthDataset(tokenizer, shuffled_dataset, num_of_sequences=10)\n",
        "dataset_iterator = iter(constant_length_dataset)\n",
        "\n",
        "lengths = [len(b) for _, b in zip(range(5), dataset_iterator)]\n",
        "print(f\"Lengths of the sequences: {lengths}\")"
      ],
      "metadata": {
        "id": "Cb9QK4Wja9uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "\n",
        "config = {\"train_batch_size\": 12,\n",
        "          \"valid_batch_size\": 12,\n",
        "          \"weight_decay\": 0.1,\n",
        "          \"shuffle_buffer\": 1000,\n",
        "          \"learning_rate\": 5e-4,\n",
        "          \"lr_scheduler_type\": \"cosine\",\n",
        "          \"num_warmup_steps\": 2000,\n",
        "          \"gradient_accumulation_steps\": 1,\n",
        "          \"max_train_steps\": 150000,\n",
        "          \"max_eval_steps\": -1,\n",
        "          \"seq_length\": 1024,\n",
        "          \"seed\": 1,\n",
        "          \"save_checkpoint_steps\": 15000}\n",
        "\n",
        "args = Namespace(**config)"
      ],
      "metadata": {
        "id": "BIhSuFgndHl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "7VcpLGDNfbp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import logging\n",
        "import wandb\n",
        "\n",
        "accelerator = Accelerator()\n",
        "\n",
        "def setup_logging(project_name):\n",
        "  logger = logging.getLogger(__name__)\n",
        "  logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "                      datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "                      level=logging.INFO,\n",
        "                      # handlers=[logging.FileHandler(f\"log/debug_{accelerator.process_index}.log\"), logging.StreamHandler()])\n",
        "                      handlers=[logging.StreamHandler()])\n",
        "  if accelerator.is_main_process:\n",
        "    wandb.init(project=project_name, config=args)\n",
        "    run_name = wandb.run.name\n",
        "    tb_writer = SummaryWriter()\n",
        "    tb_writer.add_hparams(vars(args), {'0': 0})\n",
        "    logger.setLevel(logging.INFO)\n",
        "    datasets.utils.logging.set_verbosity_debug()\n",
        "    transformers.utils.logging.set_verbosity_debug()\n",
        "  else:\n",
        "    tb_writer = None\n",
        "    run_name = \"\"\n",
        "    logger.setLevel(logging.ERROR)\n",
        "    datasets.utils.logging.set_verbosity_debug()\n",
        "    transformers.utils.logging.set_verbosity_debug()\n",
        "\n",
        "  return logger, tb_writer, run_name\n",
        "\n",
        "def log_metrics(step, metrics):\n",
        "  logger.info(f\"Step {step}: {metrics}\")\n",
        "  if accelerator.is_main_process:\n",
        "    wandb.log(metrics)\n",
        "    [tb_writer.add_scalar(k, v, step) for k, v in metrics.items()]"
      ],
      "metadata": {
        "id": "zqE-vRleeZpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "def create_dataloaders(dataset_name):\n",
        "  train_data = load_dataset(dataset_name+'-train', split=\"train\", streaming=True)\n",
        "  train_data = train_data.shuffle(buffer_size=args.shuffle_buffer, seed=args.seed)\n",
        "  valid_data = load_dataset(dataset_name+'-valid', split=\"validation\", streaming=True)\n",
        "\n",
        "  train_dataset = ConstantLengthDataset(tokenizer, train_data, seq_length=args.seq_length)\n",
        "  valid_dataset = ConstantLengthDataset(tokenizer, valid_data, seq_length=args.seq_length)\n",
        "\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=args.train_batch_size)\n",
        "  eval_dataloader = DataLoader(valid_dataset, batch_size=args.valid_batch_size)\n",
        "  return train_dataloader, eval_dataloader"
      ],
      "metadata": {
        "id": "EEtg6hYetl0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
        "  params_with_wd, params_without_wd = [], []\n",
        "  for n, p in model.named_parameters():\n",
        "    if any(nd in n for nd in no_decay):\n",
        "      params_without_wd.append(p)\n",
        "    else:\n",
        "      params_with_wd.append(p)\n",
        "  \n",
        "  return [{\"params\": params_with_wd, 'weight_decay': args.weight_decay},\n",
        "          {'params': params_without_wd, 'weight_decay': 0.0}]"
      ],
      "metadata": {
        "id": "aMnujKbcx0ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate():\n",
        "  model.eval()\n",
        "  losses = []\n",
        "  for step, batch in enumerate(eval_dataloader):\n",
        "    with torch.no_grad():\n",
        "      outputs = model(batch, labels=batch)\n",
        "    loss = outputs.loss.repeat(args.valid_batch_size)\n",
        "    losses.append(accelerator.gather(loss))\n",
        "    if args.max_eval_steps > 0 and step >= args.max_eval_steps: break\n",
        "\n",
        "  loss = torch.mean(torch.cat(losses))\n",
        "  try:\n",
        "    perplexity = torch.exp(loss)\n",
        "  except OverflowError:\n",
        "\t  perplexity = torch.tensor(float(\"inf\"))\n",
        "   \n",
        "  return loss.item(), perplexity.item()"
      ],
      "metadata": {
        "id": "4kwIxTjQy9Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AAAAA\n",
        "from transformers.utils.dummy_pt_objects import AdamW\n",
        "from transformers import set_seed\n",
        "\n",
        "set_seed(args.seed)\n",
        "\n",
        "accelerator = Accelerator()\n",
        "samples_per_step = accelerator.state.num_processes * args.train_batch_size\n",
        "\n",
        "logger, tb_writer, run_name = setup_logging(project_name.split(\"/\")[1])\n",
        "logger.info(accelerator.state)\n",
        "\n",
        "train_dataloader, eval_dataloader = create_dataloaders(\"transformersbook/codeparrot-train\")\n",
        "\n",
        "optimizer = AdamW(get_grouped_params(model), lr=args.learning_rate)\n",
        "lr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer,\n",
        "                             num_warmup_steps=args.num_warmup_steps,\n",
        "                             num_training_steps=args.max_train_steps)\n",
        "def get_lr():\n",
        "  return optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "# Prepare everything with our `accelerator`\n",
        "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n",
        "\n",
        "# train\n",
        "model.train()\n",
        "completed_steps = 0\n",
        "for step, batch in enumerate(train_dataloader, start=1):\n",
        "  loss = model(batch, labels=batch).loss\n",
        "  log_metrics(step, {\"lr\": get_lr(), \"samples\": step * samples_per_step,\n",
        "                     \"steps\": completed_steps, \"loss/train\": loss.item()})\n",
        "  loss = loss / args.gradient_accumulation_steps\n",
        "  accelerator.backward(loss)\n",
        "\n",
        "  if step % args.gradient_accumulation_steps == 0:\n",
        "    optimizer.step()\n",
        "    lr_scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    completed_steps += 1\n",
        "  if step % args.save_checkpoint_steps == 0:\n",
        "    logger.info(\"Evaluating model checkpoint\")\n",
        "    eval_loss, perplexity = evaluate()\n",
        "    log_metrics(step, {\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n",
        "    accelerator.wait_for_everyone()\n",
        "    unwrapped_model = accelerator.unwrap_model(model)\n",
        "    if accelerator.is_main_process:\n",
        "      unwrapped_model.save_pretrained(\"./\")\n",
        "    model.train()\n",
        "  \n",
        "  if completed_steps >= args.max_train_steps:\n",
        "    break\n",
        "\n",
        "# the last checkpoint\n",
        "logger.info(\"Evaluating model checkpoint\")\n",
        "eval_loss, perplexity = evaluate()\n",
        "log_metrics(step, {\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n",
        "accelerator.wait_for_everyone()\n",
        "unwrapped_model = accelerator.unwrap_model(model)\n",
        "if accelerator.is_main_process:\n",
        "  unwrapped_model.save_pretrained(\"./\")"
      ],
      "metadata": {
        "id": "z7r1eJ9r2Scj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "\n",
        "ckpt = \"transformersbook/codeparrot-small\"\n",
        "generation = pipeline(\"text-generation\", model=ckpt)"
      ],
      "metadata": {
        "id": "15ttVv03Jws6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def first_block(string):\n",
        "  return re.split('\\nclass|\\ndef|\\n#|\\n@|\\nprint|\\nif', string)[0].rstrip()\n",
        "\n",
        "def complete_code(pipe, prompt, max_length=64, num_completions=4, seed=1):\n",
        "  set_seed(seed)\n",
        "  gen_kwargs = {\"temperature\":0.4, \"top_p\":0.95, \"top_k\":0, \"num_beams\":1, \"do_sample\":True}\n",
        "  code_gens = generation(prompt, num_return_sequences=num_completions, max_length=max_length, **gen_kwargs)\n",
        "  code_strings = []\n",
        "  for code_gen in code_gens:\n",
        "    generated_code = first_block(code_gen['generated_text'][len(prompt):])\n",
        "    code_strings.append(generated_code)\n",
        "  print(('\\n'+'='*80 + '\\n').join(code_strings))"
      ],
      "metadata": {
        "id": "nZiOyTwNKZFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = '''def area_of_rectangle(a: float, b: float):\n",
        "         \"\"\"Return the area of the rectangle.\"\"\"'''\n",
        "complete_code(generation, prompt)"
      ],
      "metadata": {
        "id": "EkpoS5zmLxKk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}