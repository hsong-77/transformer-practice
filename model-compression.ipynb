{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8mbjJcTwzLeJlgZYcDFJG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsong-77/transformer-practice/blob/main/model-compression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvkrTRsxcEVD"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "clinc = load_dataset(\"clinc_oos\", \"plus\")\n",
        "clinc"
      ],
      "metadata": {
        "id": "D6BbrdsW7U6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = clinc[\"test\"][42]\n",
        "sample"
      ],
      "metadata": {
        "id": "8cje8eP58Mjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intents = clinc[\"test\"].features[\"intent\"]\n",
        "intents.int2str(sample[\"intent\"])"
      ],
      "metadata": {
        "id": "3ZROBXKn8QXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\n",
        "pipe = pipeline(\"text-classification\", model=ckpt)"
      ],
      "metadata": {
        "id": "ihvIbEGy4Qev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"Hey, I'd like to rent a vehicle from Nov 1st to Nov 15th in Paris and I need a 15 passenger van\"\"\"\n",
        "pipe(query)"
      ],
      "metadata": {
        "id": "0nkLcqum45dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "from pathlib import Path\n",
        "from time import perf_counter\n",
        "\n",
        "accuracy_score = load_metric(\"accuracy\")\n",
        "\n",
        "class PerformanceBenchmark:\n",
        "  def __init__(self, pipeline, dataset, optim_type=\"BERT baseline\"):\n",
        "    self.pipeline = pipeline\n",
        "    self.dataset = dataset\n",
        "    self.optim_type = optim_type\n",
        "\n",
        "  def compute_accuracy(self):\n",
        "    preds, labels = [], []\n",
        "    for example in self.dataset:\n",
        "      pred = self.pipeline(example[\"text\"])[0][\"label\"]\n",
        "      label = example[\"intent\"]\n",
        "      preds.append(intents.str2int(pred))\n",
        "      labels.append(label)\n",
        "\n",
        "    accuracy = accuracy_score.compute(predictions=preds, references=labels)\n",
        "    print(f\"Accuracy on test set - {accuracy['accuracy']:.3f}\")\n",
        "    return accuracy\n",
        "\n",
        "  def compute_size(self):\n",
        "    state_dict = self.pipeline.model.state_dict()\n",
        "    tmp_path = Path(\"model.pt\")\n",
        "    torch.save(state_dict, tmp_path)\n",
        "    size_mb = Path(tmp_path).stat().st_size / (1024 * 1024)\n",
        "    # delete tmp path\n",
        "    tmp_path.unlink()\n",
        "\n",
        "    print(f\"Model size (MB) - {size_mb:.2f}\")\n",
        "    return {\"size_mb\": size_mb}\n",
        "\n",
        "  def time_pipeline(self, query):\n",
        "    latencies = []\n",
        "    # warm up\n",
        "    for _ in range(10):\n",
        "      _ = self.pipeline(query)\n",
        "      # timed run\n",
        "    for _ in range(100):\n",
        "      start_time = perf_counter()\n",
        "      _ = self.pipeline(query)\n",
        "      latency = perf_counter() - start_time\n",
        "      latencies.append(latency)\n",
        "    \n",
        "    time_avg_ms = 1000 * np.mean(latencies)\n",
        "    time_std_ms = 1000 * np.std(latencies)\n",
        "    print(f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\")\n",
        "    return {\"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms}\n",
        "\n",
        "  def run_benchmark(self, query):\n",
        "    metrics = {}\n",
        "    metrics[self.optim_type] = self.compute_size()\n",
        "    metrics[self.optim_type].update(self.time_pipeline(query))\n",
        "    metrics[self.optim_type].update(self.compute_accuracy())\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "Lrgn2cJe53uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pb = PerformanceBenchmark(pipe, clinc[\"test\"])\n",
        "perf_metrics = pb.run_benchmark(\"What is the pin number for my account?\")"
      ],
      "metadata": {
        "id": "houMKbDc0IpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "class DistillationTrainingArguments(TrainingArguments):\n",
        "  def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.alpha = alpha\n",
        "    self.temperature = temperature"
      ],
      "metadata": {
        "id": "eU2_GX4aB1vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import Trainer\n",
        "\n",
        "class DistillationTrainer(Trainer):\n",
        "  def __init__(self, *args, teacher_model=None, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.teacher_model = teacher_model\n",
        "\n",
        "  def compute_loss(self, model, inputs, return_outputs=False):\n",
        "    outputs_stu = model(**inputs)\n",
        "    loss_ce = outputs_stu.loss\n",
        "    logits_stu = outputs_stu.logits\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs_tea = self.teacher_model(**inputs)\n",
        "      logits_tea = outputs_tea.logits\n",
        "    loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "    loss_kd = self.args.temperature ** 2 * loss_fct(F.log_softmax(logits_stu / self.args.temperature, dim=-1), F.softmax(logits_tea / self.args.temperature, dim=-1))\n",
        "\n",
        "    loss = self.args.alpha * loss_ce + (1 - self.args.alpha) * loss_kd\n",
        "    return (loss, outputs_stu) if return_outputs else loss"
      ],
      "metadata": {
        "id": "qyAj65fNCpAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "student_ckpt = \"distilbert-base-uncased\"\n",
        "student_tokenizer = AutoTokenizer.from_pretrained(student_ckpt)\n",
        "\n",
        "def tokenize_text(batch):\n",
        "  return student_tokenizer(batch[\"text\"], truncation=True)\n",
        "\n",
        "clinc_enc = clinc.map(tokenize_text, batched=True, remove_columns=[\"text\"])\n",
        "clinc_enc = clinc_enc.rename_column(\"intent\", \"labels\")\n",
        "clinc_enc"
      ],
      "metadata": {
        "id": "HR4O_m39GiCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "  predictions, labels = pred\n",
        "  predictions = np.argmax(predictions, axis=-1)\n",
        "  return accuracy_score.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "XKCbo8dpH-0B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}