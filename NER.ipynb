{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMR8LZHN8SU+vtfH6Pei7GC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsong-77/transformer-practice/blob/main/NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FitwZcD3ZbAl"
      },
      "outputs": [],
      "source": [
        "!pip install transformers===4.11.3\n",
        "!pip install datasets\n",
        "!pip install seqeval==1.2.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import get_dataset_config_names\n",
        "\n",
        "xtreme_subsets = get_dataset_config_names(\"xtreme\")\n",
        "print(f\"XTREME has {len(xtreme_subsets)} configurations\")"
      ],
      "metadata": {
        "id": "I50JkZhXXvOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from collections import defaultdict\n",
        "from datasets import DatasetDict\n",
        "\n",
        "langs = [\"de\", \"fr\", \"it\", \"en\"]\n",
        "fracs = [0.629, 0.229, 0.084, 0.059]\n",
        "panx_ch = defaultdict(DatasetDict)\n",
        "\n",
        "for lang, frac in zip(langs, fracs):\n",
        "  ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
        "  for split in ds:\n",
        "    panx_ch[lang][split] = ds[split].shuffle(seed=0).select(range(int(frac * ds[split].num_rows)))\n",
        "\n",
        "panx_ch"
      ],
      "metadata": {
        "id": "WiKaHxy3ZNLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame({lang: [panx_ch[lang][\"train\"].num_rows] for lang in langs}, index=[\"Number of training examples\"])"
      ],
      "metadata": {
        "id": "rUH3PmSDbntV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\n",
        "print(tags)"
      ],
      "metadata": {
        "id": "KvdJTAlBeF4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tag_names(batch):\n",
        "  return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\n",
        "\n",
        "panx_de = panx_ch[\"de\"].map(create_tag_names)"
      ],
      "metadata": {
        "id": "S2qloGG6ePnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "split2freqs = defaultdict(Counter)\n",
        "for split, dataset in panx_de.items():\n",
        "  for row in dataset[\"ner_tags_str\"]:\n",
        "    for tag in row:\n",
        "      if tag.startswith(\"B\"):\n",
        "        tag_type = tag.split(\"-\")[1]\n",
        "        split2freqs[split][tag_type] += 1\n",
        "pd.DataFrame.from_dict(split2freqs, orient=\"index\")"
      ],
      "metadata": {
        "id": "CVhdzs4OfgSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "bert_model_name = \"bert-base-cased\"\n",
        "xlmr_model_name = \"xlm-roberta-base\"\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)"
      ],
      "metadata": {
        "id": "yPV_6ugnoyTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from transformers import XLMRobertaConfig\n",
        "from transformers.modeling_outputs import TokenClassifierOutput\n",
        "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
        "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
        "\n",
        "class XLMRobertaForTokenClassificaiton(RobertaPreTrainedModel):\n",
        "  config_class = XLMRobertaConfig\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__(config)\n",
        "    self.num_labels = config.num_labels\n",
        "    # model\n",
        "    self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "    # classification head\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "    self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "    # weights\n",
        "    self.init_weights()\n",
        "\n",
        "  def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
        "    # encode\n",
        "    outputs = self.roberta(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, **kwargs)\n",
        "    # classify\n",
        "    sequence_output = self.dropout(outputs[0])\n",
        "    logits = self.classifier(sequence_output)\n",
        "    # losses\n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "      loss_fct = nn.CrossEntropyLoss()\n",
        "      loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "    #output\n",
        "    return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)"
      ],
      "metadata": {
        "id": "BMsq1AnOuRX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
        "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}\n",
        "\n",
        "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name,\n",
        "                                         num_labels=tags.num_classes,\n",
        "                                         id2label=index2tag, label2id=tag2index)"
      ],
      "metadata": {
        "id": "XGGhmyLqFRK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "xlmr_model = XLMRobertaForTokenClassificaiton.from_pretrained(xlmr_model_name, config=xlmr_config).to(device)"
      ],
      "metadata": {
        "id": "xKcrJqxyGmxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tag_text(text, tags, model, tokenizer):\n",
        "  tokens = tokenizer(text).tokens()\n",
        "  input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
        "  outputs = model(input_ids)[0]\n",
        "  predictions = torch.argmax(outputs, dim=2)\n",
        "  preds = [tags.name[p] for p in predictions[0].cpu().numpy()]\n",
        "  return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])"
      ],
      "metadata": {
        "id": "IWGBJGgX9UdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "  tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "  labels = []\n",
        "  for idx, label in enumerate(examples[\"ner_tags\"]):\n",
        "    word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
        "    previous_word_idx = None\n",
        "    label_ids = []\n",
        "    for word_idx in word_ids:\n",
        "      if word_idx is None or word_idx == previous_word_idx:\n",
        "        label_ids.append(-100)\n",
        "      else:\n",
        "        label_ids.append(label[word_idx])\n",
        "      previous_word_idx = word_idx\n",
        "    labels.append(label_ids)\n",
        "    \n",
        "  tokenized_inputs[\"labels\"] = labels\n",
        "  return tokenized_inputs"
      ],
      "metadata": {
        "id": "ESYJgNEnEnBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_panx_dataset(corpus):\n",
        "  return corpus.map(tokenize_and_align_labels, batched=True, remove_columns=['langs', 'ner_tags', 'tokens'])\n",
        "\n",
        "panx_de_encoded = encode_panx_dataset(panx_ch[\"de\"])\n",
        "panx_de_encoded"
      ],
      "metadata": {
        "id": "2aydTqzPKsay"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}