{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1JYzSRXLsBVoQxIh3U/xA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsong-77/transformer-practice/blob/main/NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FitwZcD3ZbAl"
      },
      "outputs": [],
      "source": [
        "!pip install transformers===4.11.3\n",
        "!pip install datasets\n",
        "!pip install seqeval==1.2.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import get_dataset_config_names\n",
        "\n",
        "xtreme_subsets = get_dataset_config_names(\"xtreme\")\n",
        "print(f\"XTREME has {len(xtreme_subsets)} configurations\")"
      ],
      "metadata": {
        "id": "I50JkZhXXvOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from collections import defaultdict\n",
        "from datasets import DatasetDict\n",
        "\n",
        "langs = [\"de\", \"fr\", \"it\", \"en\"]\n",
        "fracs = [0.629, 0.229, 0.084, 0.059]\n",
        "panx_ch = defaultdict(DatasetDict)\n",
        "\n",
        "for lang, frac in zip(langs, fracs):\n",
        "  ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
        "  for split in ds:\n",
        "    panx_ch[lang][split] = ds[split].shuffle(seed=0).select(range(int(frac * ds[split].num_rows)))\n",
        "\n",
        "panx_ch"
      ],
      "metadata": {
        "id": "WiKaHxy3ZNLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame({lang: [panx_ch[lang][\"train\"].num_rows] for lang in langs}, index=[\"Number of training examples\"])"
      ],
      "metadata": {
        "id": "rUH3PmSDbntV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\n",
        "print(tags)"
      ],
      "metadata": {
        "id": "KvdJTAlBeF4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tag_names(batch):\n",
        "  return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\n",
        "\n",
        "panx_de = panx_ch[\"de\"].map(create_tag_names)"
      ],
      "metadata": {
        "id": "S2qloGG6ePnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "split2freqs = defaultdict(Counter)\n",
        "for split, dataset in panx_de.items():\n",
        "  for row in dataset[\"ner_tags_str\"]:\n",
        "    for tag in row:\n",
        "      if tag.startswith(\"B\"):\n",
        "        tag_type = tag.split(\"-\")[1]\n",
        "        split2freqs[split][tag_type] += 1\n",
        "pd.DataFrame.from_dict(split2freqs, orient=\"index\")"
      ],
      "metadata": {
        "id": "CVhdzs4OfgSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "bert_model_name = \"bert-base-cased\"\n",
        "xlmr_model_name = \"xlm-roberta-base\"\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)"
      ],
      "metadata": {
        "id": "yPV_6ugnoyTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from transformers import XLMRobertaConfig\n",
        "from transformers.modeling_outputs import TokenClassifierOutput\n",
        "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
        "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
        "\n",
        "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
        "  config_class = XLMRobertaConfig\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__(config)\n",
        "    self.num_labels = config.num_labels\n",
        "    # model\n",
        "    self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "    # classification head\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "    self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "    # weights\n",
        "    self.init_weights()\n",
        "\n",
        "  def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
        "    # encode\n",
        "    outputs = self.roberta(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, **kwargs)\n",
        "    # classify\n",
        "    sequence_output = self.dropout(outputs[0])\n",
        "    logits = self.classifier(sequence_output)\n",
        "    # losses\n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "      loss_fct = nn.CrossEntropyLoss()\n",
        "      loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "    #output\n",
        "    return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)"
      ],
      "metadata": {
        "id": "BMsq1AnOuRX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
        "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}\n",
        "\n",
        "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name,\n",
        "                                         num_labels=tags.num_classes,\n",
        "                                         id2label=index2tag, label2id=tag2index)"
      ],
      "metadata": {
        "id": "XGGhmyLqFRK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "xlmr_model = XLMRobertaForTokenClassification.from_pretrained(xlmr_model_name, config=xlmr_config).to(device)"
      ],
      "metadata": {
        "id": "xKcrJqxyGmxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tag_text(text, tags, model, tokenizer):\n",
        "  tokens = tokenizer(text).tokens()\n",
        "  input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
        "  outputs = model(input_ids)[0]\n",
        "  predictions = torch.argmax(outputs, dim=2)\n",
        "  preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
        "  return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])"
      ],
      "metadata": {
        "id": "IWGBJGgX9UdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "  tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "  labels = []\n",
        "  for idx, label in enumerate(examples[\"ner_tags\"]):\n",
        "    word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
        "    previous_word_idx = None\n",
        "    label_ids = []\n",
        "    for word_idx in word_ids:\n",
        "      if word_idx is None or word_idx == previous_word_idx:\n",
        "        label_ids.append(-100)\n",
        "      else:\n",
        "        label_ids.append(label[word_idx])\n",
        "      previous_word_idx = word_idx\n",
        "    labels.append(label_ids)\n",
        "    \n",
        "  tokenized_inputs[\"labels\"] = labels\n",
        "  return tokenized_inputs"
      ],
      "metadata": {
        "id": "ESYJgNEnEnBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_panx_dataset(corpus):\n",
        "  return corpus.map(tokenize_and_align_labels, batched=True, remove_columns=['langs', 'ner_tags', 'tokens'])\n",
        "\n",
        "panx_de_encoded = encode_panx_dataset(panx_ch[\"de\"])\n",
        "panx_de_encoded"
      ],
      "metadata": {
        "id": "2aydTqzPKsay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report\n",
        "\n",
        "y_true = [[\"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n",
        "          [\"B-PER\", \"I-PER\", \"O\"]]\n",
        "y_pred = [[\"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n",
        "          [\"B-PER\", \"I-PER\", \"O\"]]\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "id": "a7aFu4d-_BXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def align_predicitions(predictions, label_ids):\n",
        "  preds = np.argmax(predictions, aixs=2)\n",
        "  batch_size, seq_len = preds.shape\n",
        "\n",
        "  labels_list, preds_list = [], []\n",
        "  for batch_idx in range(batch_size):\n",
        "    example_labels, example_preds = [], []\n",
        "    for seq_idx in range(seq_len):\n",
        "      if label_ids[batch_idx, seq_idx] != -100:\n",
        "        example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
        "        example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
        "    labels_list.append(example_labels)\n",
        "    preds_list.append(example_preds)\n",
        "\n",
        "  return preds_list, labels_list"
      ],
      "metadata": {
        "id": "dC7wPOXP_N7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "num_epochs = 1\n",
        "batch_size = 24\n",
        "logging_steps = len(panx_de_encoded[\"train\"]) // batch_size\n",
        "model_name = f\"{xlmr_model_name}-finetuned-panx-de\"\n",
        "training_args = TrainingArguments(output_dir=model_name, log_level=\"error\", num_train_epochs=num_epochs,\n",
        "                                  per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size,\n",
        "                                  evaluation_strategy=\"epoch\", save_steps=1e6, weight_decay=0.01, disable_tqdm=False,\n",
        "                                  logging_steps=logging_steps, push_to_hub=False)"
      ],
      "metadata": {
        "id": "byknJikQCqEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import f1_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "  y_pred, y_true = align_predicitions(eval_pred.predictions, eval_pred.label_ids)\n",
        "  return {\"f1\": f1_score(y_true, y_pred)}"
      ],
      "metadata": {
        "id": "Euq4K5TDEfaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)"
      ],
      "metadata": {
        "id": "uqsKL8Q2FxN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_init():\n",
        "    return XLMRobertaForTokenClassification.from_pretrained(xlmr_model_name, config=xlmr_config).to(device)"
      ],
      "metadata": {
        "id": "tU-JG-a-GxhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(model_init=model_init, args=training_args,\n",
        "                  data_collator=data_collator, compute_metrics=compute_metrics,\n",
        "                  train_dataset=panx_de_encoded[\"train\"],\n",
        "                  eval_dataset=panx_de_encoded[\"validation\"],\n",
        "                  tokenizer=xlmr_tokenizer)\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "9y3U_zNiIKxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_de = \"Jeff Dean ist ein Informatiker bei Google in Kalifornien\"\n",
        "tag_text(text_de, tags, trainer.model, xlmr_tokenizer)"
      ],
      "metadata": {
        "id": "vslewhTlJGE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import cross_entropy\n",
        "\n",
        "def forward_pass_with_label(batch):\n",
        "  features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
        "  batch = data_collator(features)\n",
        "  input_ids = batch[\"input_ids\"].to(device)\n",
        "  attention_mask = batch[\"attention_mask\"].to(device)\n",
        "  labels = batch[\"labels\"].to(device)\n",
        "    \n",
        "  with torch.no_grad():\n",
        "    output = trainer.model(input_ids, attention_mask)\n",
        "    # logit.size: [batch_size, sequence_length, classes]\n",
        "    predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy()\n",
        "\n",
        "  loss = cross_entropy(output.logits.view(-1, 7), labels.view(-1), reduction=\"none\")\n",
        "  loss = loss.view(len(input_ids), -1).cpu().numpy()\n",
        "\n",
        "  return {\"loss\":loss, \"predicted_label\": predicted_label}"
      ],
      "metadata": {
        "id": "YJA9ac4ivbPC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}