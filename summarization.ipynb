{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXqpCWFKQRh91R8am+9K9/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsong-77/transformer-practice/blob/main/summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJCyy0eRnZ4x"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.11.3\n",
        "!pip install datasets==1.16.1\n",
        "!pip install nltk==3.6.6\n",
        "!pip install sacrebleu==1.5.1\n",
        "!pip install rouge-score==0.0.4\n",
        "!pip install py7zr # Needed for samsum dataset\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"ccdv/cnn_dailymail\", version=\"3.0.0\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "vPeqzQrlpbC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample\n",
        "sample = dataset[\"train\"][1]\n",
        "print(f\"\"\"Article (excerpt of 500 characters, total length: {len(sample[\"article\"])}):\"\"\")\n",
        "print(sample[\"article\"][:500])\n",
        "print(f'\\nSummary (length: {len(sample[\"highlights\"])}):')\n",
        "print(sample[\"highlights\"])"
      ],
      "metadata": {
        "id": "9Oymn9F-Txyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = dataset[\"train\"][1][\"article\"][:2000]\n",
        "summaries = {}"
      ],
      "metadata": {
        "id": "zh26i-0rV6xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "id": "WcYZpPlGUFQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline\n",
        "def three_sentence_summary(text):\n",
        "  return \"\\n\".join(sent_tokenize(text)[:3])\n",
        "\n",
        "summaries[\"baseline\"] = three_sentence_summary(sample_text)"
      ],
      "metadata": {
        "id": "3pfSyrRfVCWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gpt2\n",
        "from transformers import pipeline, set_seed\n",
        "\n",
        "set_seed(42)\n",
        "pipe = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "gpt2_query = sample_text + \"\\nTL;DR:\\n\"\n",
        "pipe_out = pipe(gpt2_query, max_length=512, clean_up_tokenization_spaces=True)\n",
        "summaries[\"gpt2\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"generated_text\"][len(gpt2_query):]))"
      ],
      "metadata": {
        "id": "dNAD9tdyWcZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# t5\n",
        "pipe = pipeline(\"summarization\", model=\"t5-small\")\n",
        "pipe_out = pipe(sample_text)\n",
        "summaries[\"t5\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))"
      ],
      "metadata": {
        "id": "ErO2g57lJgCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bart\n",
        "pipe = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "pipe_out = pipe(sample_text)\n",
        "summaries[\"bart\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))"
      ],
      "metadata": {
        "id": "TbijKthCLcFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pegasus\n",
        "pipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\", use_fast=False)\n",
        "pipe_out = pipe(sample_text)\n",
        "summaries[\"pegasus\"] = pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\")"
      ],
      "metadata": {
        "id": "-7bViRrBO9PL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare\n",
        "print(\"GROUND TRUTH\")\n",
        "print(dataset[\"train\"][1][\"highlights\"])\n",
        "print(\"\")\n",
        "\n",
        "for model_name in summaries:\n",
        "  print(model_name.upper())\n",
        "  print(summaries[model_name])\n",
        "  print(\"\")"
      ],
      "metadata": {
        "id": "lyrBoyRMPRg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bleu\n",
        "from datasets import load_metric\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "bleu_metric = load_metric(\"sacrebleu\")\n",
        "\n",
        "bleu_metric.add(prediction=\"the cat is on mat\", reference=[\"the cat is on the mat\"])\n",
        "results = bleu_metric.compute(smooth_method=\"floor\", smooth_value=0)\n",
        "results[\"precisions\"] = [np.round(p, 2) for p in results[\"precisions\"]]\n",
        "pd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Value\"])"
      ],
      "metadata": {
        "id": "htePAMlMTGcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rouge\n",
        "rouge_metric = load_metric(\"rouge\")\n",
        "\n",
        "reference = dataset[\"train\"][1][\"highlights\"]\n",
        "records = []\n",
        "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
        "\n",
        "for model_name in summaries:\n",
        "  rouge_metric.add(prediction=summaries[model_name], reference=reference)\n",
        "  score = rouge_metric.compute()\n",
        "  rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
        "  records.append(rouge_dict)\n",
        "\n",
        "pd.DataFrame.from_records(records, index=summaries.keys())"
      ],
      "metadata": {
        "id": "_vcfhbOd_S5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def chunks(list_of_elements, batch_size):\n",
        "  for i in range(0, len(list_of_elements), batch_size):\n",
        "    yield list_of_elements[i:i+batch_size]\n",
        "\n",
        "def evaluate_summaries_pegasus(dataset, metric, model, tokenizer,\n",
        "                               batch_size=16, device=device,\n",
        "                               column_text=\"article\", column_summary=\"highlights\"):\n",
        "  article_batches = list(chunks(dataset[column_text], batch_size))\n",
        "  target_batches = list(chunks(dataset[column_summary], batch_size))\n",
        "\n",
        "  for article_batch, target_batch in tqdm(zip(article_batches, target_batches), total=len(article_batches)):\n",
        "    inputs = tokenizer(article_batch, max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
        "                               attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "                               length_penalty=0.8, num_beams=8, max_length=128)\n",
        "    decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=True) for s in summaries]\n",
        "    decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\n",
        "\n",
        "    metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
        "\n",
        "  score = metric.compute()\n",
        "  return score"
      ],
      "metadata": {
        "id": "ivUe2cmJSA7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pegasus\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "test_sampled = dataset[\"test\"].shuffle(seed=42).select(range(1000))\n",
        "\n",
        "ckpt = \"google/pegasus-cnn_dailymail\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(ckpt)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(ckpt).to(device)\n",
        "score = evaluate_summaries_pegasus(test_sampled, rouge_metric, model, tokenizer, batch_size=8)\n",
        "\n",
        "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
        "pd.DataFrame(rouge_dict, index=[\"pegasus\"])"
      ],
      "metadata": {
        "id": "egecB_F2W-Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transfer: samsum data\n",
        "dataset_samsum = load_dataset(\"samsum\")\n",
        "split_lengths = [len(dataset_samsum[split])for split in dataset_samsum]\n",
        "\n",
        "print(f\"Split lengths: {split_lengths}\")\n",
        "print(f\"Features: {dataset_samsum['train'].column_names}\")\n",
        "print(\"\\nDialogue:\")\n",
        "print(dataset_samsum[\"test\"][0][\"dialogue\"])\n",
        "print(\"\\nSummary:\")\n",
        "print(dataset_samsum[\"test\"][0][\"summary\"])"
      ],
      "metadata": {
        "id": "9TVAH1YpZtOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = evaluate_summaries_pegasus(dataset_samsum[\"test\"], rouge_metric, model, tokenizer, column_text=\"dialogue\", column_summary=\"summary\", batch_size=8)\n",
        "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
        "pd.DataFrame(rouge_dict, index=[\"pegasus\"])"
      ],
      "metadata": {
        "id": "dcMHtc7vcfRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data investigation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "d_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"dialogue\"]]\n",
        "s_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"summary\"]]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True)\n",
        "axes[0].hist(d_len, bins=20, color=\"C0\", edgecolor=\"C0\")\n",
        "axes[0].set_title(\"Dialogue Token Length\")\n",
        "axes[0].set_xlabel(\"Length\")\n",
        "axes[0].set_ylabel(\"Count\")\n",
        "axes[1].hist(s_len, bins=20, color=\"C0\", edgecolor=\"C0\")\n",
        "axes[1].set_title(\"Summary Token Length\")\n",
        "axes[1].set_xlabel(\"Length\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Vg-_bv6QfXvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_examples__to_features(example_batch):\n",
        "  input_encodings = tokenizer(example_batch[\"dialogue\"], max_length=1024, truncation=True)\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    target_encodings = tokenizer(example_batch[\"summary\"], max_length=128, truncation=True)\n",
        "\n",
        "  return {\"input_ids\": input_encodings[\"input_ids\"],\n",
        "          \"attention_mask\": input_encodings[\"attention_mask\"],\n",
        "          \"labels\": target_encodings[\"input_ids\"]}\n",
        "\n",
        "dataset_samsum_pt = dataset_samsum.map(convert_examples__to_features, batched=True)\n",
        "columns = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
        "dataset_samsum_pt.set_format(type=\"torch\", columns=columns)"
      ],
      "metadata": {
        "id": "Vu4lJBa_gexi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "AMAWyQlLjlU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500,\n",
        "                                  per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
        "                                  weight_decay=0.01, logging_steps=10, push_to_hub=False,\n",
        "                                  evaluation_strategy=\"steps\", eval_steps=500, save_steps=1e6,\n",
        "                                  gradient_accumulation_steps=16)"
      ],
      "metadata": {
        "id": "vXGwvz7yjxqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(model=model, args=training_args,\n",
        "                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
        "                  train_dataset=dataset_samsum_pt[\"train\"],\n",
        "                  eval_dataset=dataset_samsum_pt[\"validation\"])\n",
        "\n",
        "trainer.train()\n",
        "score = evaluate_summaries_pegasus(dataset_samsum[\"test\"], rouge_metric, trainer.model, tokenizer,\n",
        "                                   batch_size=2, column_text=\"dialogue\", column_summary=\"summary\")\n",
        "\n",
        "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
        "pd.DataFrame(rouge_dict, index=[f\"pegasus\"])"
      ],
      "metadata": {
        "id": "OprzrAipkqQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\n",
        "sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n",
        "reference = dataset_samsum[\"test\"][0][\"summary\"]\n",
        "pipe = pipeline(\"summarization\", model=trainer.model, tokenizer=trainer.tokenizer)\n",
        "\n",
        "print(\"Dialogue:\")\n",
        "print(sample_text)\n",
        "print(\"\\nReference Summary:\")\n",
        "print(reference)\n",
        "print(\"\\nModel Summary:\")\n",
        "print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])"
      ],
      "metadata": {
        "id": "p6rmpE1QnXyQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}