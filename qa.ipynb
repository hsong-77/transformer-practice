{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbEi3+wRb3G8kYLtmIZ/XG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsong-77/transformer-practice/blob/main/qa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yme5CsXnTkEu"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install farm-haystack[colab]==1.4.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import get_dataset_config_names\n",
        "\n",
        "domains = get_dataset_config_names(\"subjqa\")\n",
        "domains"
      ],
      "metadata": {
        "id": "3IuBi8KD-EFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "subjqa = load_dataset(\"subjqa\", name=\"electronics\")\n",
        "subjqa"
      ],
      "metadata": {
        "id": "NJ40eKi8-aho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "dfs = {split: dset.to_pandas() for split, dset in subjqa.flatten().items()}\n",
        "\n",
        "qa_cols = [\"title\", \"question\", \"answers.text\", \"answers.answer_start\", \"context\"]\n",
        "sample_df = dfs[\"train\"][qa_cols].sample(2, random_state=7)\n",
        "sample_df"
      ],
      "metadata": {
        "id": "3Ek862pJBjYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "counts = {}\n",
        "question_types = [\"What\", \"How\", \"Is\", \"Does\", \"Do\", \"Was\", \"Where\", \"Why\"]\n",
        "\n",
        "for q in question_types:\n",
        "  counts[q] = dfs[\"train\"][\"question\"].str.startswith(q).value_counts()[True]\n",
        "\n",
        "pd.Series(counts).sort_values().plot.barh()\n",
        "plt.title(\"Frequency of Question Types\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cd2F5C3QEI7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for question_type in [\"How\", \"What\", \"Is\"]:\n",
        "  for question in dfs[\"train\"][dfs[\"train\"].question.str.startswith(question_type)].sample(n=3, random_state=42)['question']:\n",
        "    print(question)"
      ],
      "metadata": {
        "id": "S5wuLILXE7KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "ckpt = \"deepset/minilm-uncased-squad2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(ckpt)"
      ],
      "metadata": {
        "id": "RBBfEqDmOnQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How much music can this hold?\"\n",
        "context = \"\"\"An MP3 is about 1 MB/minute, so about 6000 hours depending on \\\n",
        "file size.\"\"\"\n",
        "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "inputs"
      ],
      "metadata": {
        "id": "yXQ2rItkO-lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(inputs[\"input_ids\"][0]))"
      ],
      "metadata": {
        "id": "SdG34X0kPrFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForQuestionAnswering\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(ckpt)\n",
        "\n",
        "with torch.no_grad():\n",
        "  outputs = model(**inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "bYlNw517TXHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "start_logits = outputs.start_logits\n",
        "end_logits = outputs.end_logits\n",
        "\n",
        "start_idx = torch.argmax(start_logits)\n",
        "end_idx = torch.argmax(end_logits) + 1\n",
        "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
        "answer = tokenizer.decode(answer_span)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "FBu2RYbHUB44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "s_scores = start_logits.detach().numpy().flatten()\n",
        "e_scores = end_logits.detach().numpy().flatten()\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "token_ids = range(len(tokens))\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=2, sharex=True)\n",
        "colors = [\"C0\" if s != np.max(s_scores) else \"C1\" for s in s_scores]\n",
        "ax1.bar(x=token_ids, height=s_scores, color=colors)\n",
        "ax1.set_ylabel(\"Start Scores\")\n",
        "colors = [\"C0\" if s != np.max(e_scores) else \"C1\" for s in e_scores]\n",
        "ax2.bar(x=token_ids, height=e_scores, color=colors)\n",
        "ax2.set_ylabel(\"End Scores\")\n",
        "plt.xticks(token_ids, tokens, rotation=\"vertical\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NSHMo4NlYMBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
        "pipe(question=question, context=context, topk=3)"
      ],
      "metadata": {
        "id": "WKuIRwccVUTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_input_length(row):\n",
        "  inputs = tokenizer(row[\"question\"], row[\"context\"])\n",
        "  return len(inputs[\"input_ids\"])\n",
        "\n",
        "dfs[\"train\"][\"n_tokens\"] = dfs[\"train\"].apply(compute_input_length, axis=1)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "dfs[\"train\"][\"n_tokens\"].hist(bins=100, grid=False, ec=\"C0\", ax=ax)\n",
        "plt.xlabel(\"Number of tokens in question-context pair\")\n",
        "ax.axvline(x=512, ymin=0, ymax=1, linestyle=\"--\", color=\"C1\", \n",
        "           label=\"Maximum sequence length\")\n",
        "plt.legend()\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dJGogclMYYVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = dfs[\"train\"].iloc[0][[\"question\", \"context\"]]\n",
        "tokenized_example = tokenizer(example[\"question\"], example[\"context\"], return_overflowing_tokens=True, max_length=100, stride=25)\n",
        "\n",
        "for idx, window in enumerate(tokenized_example[\"input_ids\"]):\n",
        "  print(f\"Window #{idx} has {len(window)} tokens\")\n",
        "\n",
        "for window in tokenized_example[\"input_ids\"]:\n",
        "  print(f\"{tokenizer.decode(window)} \\n\")"
      ],
      "metadata": {
        "id": "F-u7ZmnvXN5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"\"\"https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz\"\"\"\n",
        "!wget -nc -q {url}\n",
        "!tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz"
      ],
      "metadata": {
        "id": "ODQEpCp9YkGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "\n",
        "# background process\n",
        "!chown -R daemon:daemon elasticsearch-7.9.2\n",
        "es_server = Popen(args=['elasticsearch-7.9.2/bin/elasticsearch'], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1))\n",
        "# Wait until Elasticsearch has started\n",
        "!sleep 30"
      ],
      "metadata": {
        "id": "6ixjYZXJYtpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -X GET \"localhost:9200/?pretty\""
      ],
      "metadata": {
        "id": "JDawzxqNZXrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.document_stores.elasticsearch import ElasticsearchDocumentStore\n",
        "\n",
        "document_store = ElasticsearchDocumentStore(return_embedding=True)"
      ],
      "metadata": {
        "id": "VbuOK-p-ZpN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if len(document_store.get_all_documents()) or len(document_store.get_all_labels()) > 0:\n",
        "  document_store.delete_documents(index=\"document\")\n",
        "  document_store.delete_documents(index=\"label\")"
      ],
      "metadata": {
        "id": "9KBi59xIasId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for split, df in dfs.items():\n",
        "  docs = [{\"content\": row[\"context\"], \"id\": row[\"review_id\"], \"meta\": {\"item_id\": row[\"title\"], \"question_id\": row[\"id\"], \"split\": split}}\n",
        "          for _, row in df.drop_duplicates(subset=\"context\").iterrows()]\n",
        "  document_store.write_documents(documents=docs, index=\"document\")\n",
        "\n",
        "print(f\"Loaded {document_store.get_document_count()} documents\")"
      ],
      "metadata": {
        "id": "52ZRfs8YbED-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sparse retriever\n",
        "from haystack.nodes.retriever import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever(document_store=document_store)"
      ],
      "metadata": {
        "id": "pPE9H8nQdufh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_id = \"B0074BW614\"\n",
        "query = \"Is it good for reading?\"\n",
        "retrieved_docs = bm25_retriever.retrieve(query=query, top_k=3, filters={\"item_id\": [item_id], \"split\": [\"train\"]})\n",
        "\n",
        "print(retrieved_docs[0])"
      ],
      "metadata": {
        "id": "QqSgi0pAfhK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.nodes import FARMReader\n",
        "\n",
        "ckpt = \"deepset/minilm-uncased-squad2\"\n",
        "max_seq_length = 284\n",
        "doc_stride = 128\n",
        "reader = FARMReader(model_name_or_path=ckpt, progress_bar=False, max_seq_len=max_seq_length, doc_stride=doc_stride, return_no_answer=True)\n",
        "\n",
        "print(reader.predict_on_texts(question=question, texts=[context], top_k=1))"
      ],
      "metadata": {
        "id": "cz9bibDfjIgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.pipelines import ExtractiveQAPipeline\n",
        "\n",
        "pipe = ExtractiveQAPipeline(reader=reader, retriever=bm25_retriever)"
      ],
      "metadata": {
        "id": "Do8TYKvels58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_answers = 3\n",
        "preds = pipe.run(query=query, params={\"Retriever\": {\"top_k\": 3, \"filters\": {\"item_id\": [item_id], \"split\": [\"train\"]}}, \"Reader\": {\"top_k\": n_answers}})\n",
        "\n",
        "print(f\"Question: {preds['query']} \\n\")\n",
        "for idx in range(n_answers):\n",
        "  print(f\"Answer {idx+1}: {preds['answers'][idx].answer}\")\n",
        "  print(f\"Review snippet: ...{preds['answers'][idx].context}...\")\n",
        "  print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "xhFTD1immfEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.nodes import retriever\n",
        "# Evaluating the Retriever\n",
        "from haystack.pipelines import DocumentSearchPipeline\n",
        "\n",
        "pipe = DocumentSearchPipeline(retriever=bm25_retriever)"
      ],
      "metadata": {
        "id": "_p5D8Pmiol3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack import Label, Answer, Document\n",
        "\n",
        "labels = []\n",
        "for i, row in dfs[\"test\"].iterrows():\n",
        "  # for filtering in the Retriever\n",
        "  meta = {\"item_id\": row[\"title\"], \"question_id\": row[\"id\"]}\n",
        "  # labels for questions with answers\n",
        "  if len(row[\"answers.text\"]):\n",
        "    for answer in row[\"answers.text\"]:\n",
        "      label = Label(query=row[\"question\"], answer=Answer(answer=answer), origin=\"gold-label\",\n",
        "                    document=Document(content=row[\"context\"], id=row[\"review_id\"]),\n",
        "                    meta=meta, is_correct_answer=True, is_correct_document=True,\n",
        "                    no_answer=False, filters={\"item_id\": [meta[\"item_id\"]], \"split\": [\"test\"]})\n",
        "      labels.append(label)\n",
        "  # labels for questions without answers\n",
        "  else:\n",
        "    label = Label(query=row[\"question\"], answer=Answer(answer=\"\"), origin=\"gold-label\",\n",
        "                  document=Document(content=row[\"context\"], id=row[\"review_id\"]),\n",
        "                  meta=meta, is_correct_answer=True, is_correct_document=True,\n",
        "                  no_answer=True, filters={\"item_id\": [row[\"title\"]], \"split\":[\"test\"]})\n",
        "    labels.append(label)\n",
        "\n",
        "document_store.write_labels(labels, index=\"label\")\n",
        "labels_agg = document_store.get_all_labels_aggregated(index=\"label\", open_domain=True, aggregate_by_meta=[\"item_id\"])"
      ],
      "metadata": {
        "id": "_LGTQGrGnPY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# choose top_k\n",
        "def evaluate_retriever(retriever, topk_value=[1, 3, 5, 10, 20]):\n",
        "  topk_results = {}\n",
        "  max_top_k = max(topk_value)\n",
        "  p = DocumentSearchPipeline(retriever=retriever)\n",
        "  eval_result = p.eval(labels=labels_agg, params={\"Retriever\": {\"top_k\": max_top_k}})\n",
        "\n",
        "  for topk in topk_value:\n",
        "    metrics = eval_result.calculate_metrics(simulated_top_k_retriever=topk)\n",
        "    topk_results[topk] = {\"recall\": metrics[\"Retriever\"][\"recall_single_hit\"]}\n",
        "\n",
        "  return pd.DataFrame.from_dict(topk_results, orient=\"index\")\n",
        "\n",
        "bm25_topk_df = evaluate_retriever(bm25_retriever)"
      ],
      "metadata": {
        "id": "d_yIveyWu06N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_retriever_eval(dfs, retriever_names):\n",
        "  fig, ax = plt.subplots()\n",
        "  for df, retriever_name in zip(dfs, retriever_names):\n",
        "    df.plot(y=\"recall\", ax=ax, label=retriever_name)\n",
        "  plt.xticks(df.index)\n",
        "  plt.ylabel(\"Top-k Recall\")\n",
        "  plt.xlabel(\"k\")\n",
        "  plt.show()\n",
        "    \n",
        "plot_retriever_eval([bm25_topk_df], [\"BM25\"])"
      ],
      "metadata": {
        "id": "A9FQu7sNyT6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dense retriever\n",
        "from haystack.nodes import DensePassageRetriever\n",
        "\n",
        "dpr_retriever = DensePassageRetriever(document_store=document_store,\n",
        "                                      query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
        "                                      passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
        "                                      embed_title=False)\n",
        "\n",
        "document_store.update_embeddings(retriever=dpr_retriever)"
      ],
      "metadata": {
        "id": "5N5AsABnn7wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dpr_topk_df = evaluate_retriever(dpr_retriever)\n",
        "plot_retriever_eval([bm25_topk_df, dpr_topk_df], [\"BM25\", \"DPR\"])"
      ],
      "metadata": {
        "id": "P051toCwp1it"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the Reader\n",
        "from haystack.modeling.evaluation.squad import compute_f1, compute_exact\n",
        "from haystack.pipelines import Pipeline\n",
        "\n",
        "def evaluate_reader(reader):\n",
        "  score_keys = ['exact_match', 'f1']\n",
        "  p = Pipeline()\n",
        "  p.add_node(component=reader, name=\"Reader\", inputs=[\"Query\"])\n",
        "\n",
        "  eval_result = p.eval(labels=labels_agg, documents=[[label.document for label in multilabel.labels] for multilabel in labels_agg], params={})\n",
        "  metrics = eval_result.calculate_metrics(simulated_top_k_reader=1)\n",
        "\n",
        "  return {k:v for k,v in metrics[\"Reader\"].items() if k in score_keys}\n",
        "\n",
        "reader_eval = {}\n",
        "reader_eval[\"Fine-tune on SQuAD\"] = evaluate_reader(reader)"
      ],
      "metadata": {
        "id": "STZC3OEMrIc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_reader_eval(reader_eval):\n",
        "  fig, ax = plt.subplots()\n",
        "  df = pd.DataFrame.from_dict(reader_eval).reindex([\"exact_match\", \"f1\"])\n",
        "  df.plot(kind=\"bar\", ylabel=\"Score\", rot=0, ax=ax)\n",
        "  ax.set_xticklabels([\"EM\", \"F1\"])\n",
        "  plt.legend(loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "plot_reader_eval(reader_eval)"
      ],
      "metadata": {
        "id": "TyRH6Fga-sur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# domain adaptation\n",
        "def create_paragraphs(df):\n",
        "  paragraphs = []\n",
        "  id2context = dict(zip(df[\"review_id\"], df[\"context\"]))\n",
        "  for review_id, review in id2context.items():\n",
        "    qas = []\n",
        "    review_df = df.query(f\"review_id == '{review_id}'\")\n",
        "    id2question = dict(zip(review_df[\"id\"], review_df[\"question\"]))\n",
        "\n",
        "    for qid, question in id2question.items():\n",
        "      question_df = df.query(f\"id == '{qid}'\").to_dict(orient=\"list\")\n",
        "      ans_start_idxs = question_df[\"answers.answer_start\"][0].tolist()\n",
        "      ans_text = question_df[\"answers.text\"][0].tolist()\n",
        "      if len(ans_start_idxs):\n",
        "        answers = [{\"text\": text, \"answer_start\": answer_start} for text, answer_start in zip(ans_text, ans_start_idxs)]\n",
        "        is_impossible = False\n",
        "      else:\n",
        "        answers = []\n",
        "        is_impossible = True\n",
        "      qas.append({\"question\": question, \"id\": qid, \"is_impossible\": is_impossible, \"answers\": answers})\n",
        "    \n",
        "    paragraphs.append({\"qas\": qas, \"context\": review})\n",
        "  \n",
        "  return paragraphs"
      ],
      "metadata": {
        "id": "nQyFFqqI_aDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data\n",
        "import json\n",
        "\n",
        "def convert_to_squad(dfs):\n",
        "  for split, df in dfs.items():\n",
        "    subjqa_data = {}\n",
        "    groups = (df.groupby(\"title\").apply(create_paragraphs).to_frame(name=\"paragraphs\").reset_index())\n",
        "    subjqa_data[\"data\"] = groups.to_dict(orient=\"records\")\n",
        "\n",
        "    with open(f\"electronics-{split}.json\", \"w+\", encoding=\"utf-8\") as f:\n",
        "      json.dump(subjqa_data, f)\n",
        "\n",
        "convert_to_squad(dfs)"
      ],
      "metadata": {
        "id": "TWQj4U1rJz00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find-tune\n",
        "train_filename = \"electronics-train.json\"\n",
        "dev_filename = \"electronics-validation.json\"\n",
        "\n",
        "# reader.train(data_dir=\".\", use_gpu=True, n_epochs=1, batch_size=16, train_filename=train_filename, dev_filename=dev_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFoANl1iLJrF",
        "outputId": "51e55f06-a8e7-47ba-c10a-e47d9f6ea2d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:haystack.modeling.data_handler.processor:Answer using start/end indices is '  Operation of the menus and contro' while gold label text is 'Operation of the menus and controls'.\n",
            "Example will not be converted for training/evaluation.\n",
            "WARNING:haystack.modeling.data_handler.processor:Answer using start/end indices is '  This camera performs like the pros.  Fast accurate and easy to operat' while gold label text is 'This camera performs like the pros.  Fast accurate and easy to operated'.\n",
            "Example will not be converted for training/evaluation.\n",
            "WARNING:haystack.modeling.data_handler.processor:Answer 'enjoy my music.  When I used my old one ear headset for music, the connection was not that good even if I put the phone in my pants pocket.  I used it on iPhone 4S and iPad 3 for music, phone calls, and audible app.  All Bluetooth controls work fine. ANSWERNOTFOUND' not contained in context.\n",
            "Example will not be converted for training/evaluation.\n",
            "WARNING:haystack.modeling.data_handler.processor:Answer 'is good.  First unit defective.  Directions are weak and limited, Net support just plain bad. Roku insists on registration, [I think SEVEN times for us] before asking dumb questions and forcing us to run from computer to TV and back.  This is a Roku 3, and apparently it's too new for them to handle. ANSWERNOTFOUND' not contained in context.\n",
            "Example will not be converted for training/evaluation.\n",
            "Preprocessing Dataset electronics-train.json: 100%|██████████| 1265/1265 [00:06<00:00, 195.76 Dicts/s]  \n",
            "ERROR:haystack.modeling.data_handler.processor:Unable to convert 4 samples to features. Their ids are : 595-0-0, 572-0-0, 1099-0-1, 1167-0-0\n",
            "INFO:haystack.modeling.data_handler.data_silo:\n",
            "INFO:haystack.modeling.data_handler.data_silo:LOADING DEV DATA\n",
            "INFO:haystack.modeling.data_handler.data_silo:=================\n",
            "INFO:haystack.modeling.data_handler.data_silo:Loading dev set from: electronics-validation.json\n",
            "INFO:haystack.modeling.data_handler.data_silo:Multiprocessing disabled, using a single worker to convert 252dictionaries to pytorch datasets.\n",
            "Preprocessing Dataset electronics-validation.json: 100%|██████████| 252/252 [00:00<00:00, 256.36 Dicts/s] \n",
            "ERROR:haystack.modeling.data_handler.processor:Unable to convert 4 samples to features. Their ids are : 595-0-0, 1099-0-1, 1167-0-0, 572-0-0\n",
            "INFO:haystack.modeling.data_handler.data_silo:\n",
            "INFO:haystack.modeling.data_handler.data_silo:LOADING TEST DATA\n",
            "INFO:haystack.modeling.data_handler.data_silo:=================\n",
            "INFO:haystack.modeling.data_handler.data_silo:No test set is being loaded\n",
            "INFO:haystack.modeling.data_handler.data_silo:\n",
            "INFO:haystack.modeling.data_handler.data_silo:DATASETS SUMMARY\n",
            "INFO:haystack.modeling.data_handler.data_silo:================\n",
            "INFO:haystack.modeling.data_handler.data_silo:Examples in train: 2964\n",
            "INFO:haystack.modeling.data_handler.data_silo:Examples in dev  : 605\n",
            "INFO:haystack.modeling.data_handler.data_silo:Examples in test : 0\n",
            "INFO:haystack.modeling.data_handler.data_silo:Total examples   : 3569\n",
            "INFO:haystack.modeling.data_handler.data_silo:\n",
            "INFO:haystack.modeling.data_handler.data_silo:Longest sequence length observed after clipping:     284\n",
            "INFO:haystack.modeling.data_handler.data_silo:Average sequence length after clipping: 229.7925101214575\n",
            "INFO:haystack.modeling.data_handler.data_silo:Proportion clipped:      0.5657894736842105\n",
            "INFO:haystack.modeling.data_handler.data_silo:[Haystack Tip] 56.6% of your samples got cut down to 284 tokens. Consider increasing max_seq_len. This will lead to higher memory consumption but is likely to improve your model performance\n",
            "INFO:haystack.modeling.model.optimization:Loading optimizer `AdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 1e-05}'\n",
            "INFO:haystack.modeling.model.optimization:Using scheduler 'get_linear_schedule_with_warmup'\n",
            "INFO:haystack.modeling.model.optimization:Loading schedule `get_linear_schedule_with_warmup`: '{'num_training_steps': 186, 'num_warmup_steps': 37}'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reader_eval[\"Fine-tune on SQuAD + SubjQA\"] = evaluate_reader(reader)\n",
        "plot_reader_eval(reader_eval)"
      ],
      "metadata": {
        "id": "WwZUBtqKLwsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fine-tune on SubjQA\n",
        "minilm_ckpt = \"microsoft/MiniLM-L12-H384-uncased\"\n",
        "minilm_reader = FARMReader(model_name_or_path=minilm_ckpt, progress_bar=False,\n",
        "                           max_seq_len=max_seq_length, doc_stride=doc_stride,\n",
        "                           return_no_answer=True)\n",
        "# minilm_reader.train(data_dir=\".\", use_gpu=True, n_epochs=1, batch_size=16, train_filename=train_filename, dev_filename=dev_filename)\n",
        "\n",
        "reader_eval[\"Fine-tune on SubjQA\"] = evaluate_reader(minilm_reader)\n",
        "plot_reader_eval(reader_eval)"
      ],
      "metadata": {
        "id": "1MOtJIMONkDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the Whole QA Pipeline\n",
        "from haystack.pipelines import ExtractiveQAPipeline\n",
        "\n",
        "pipe = ExtractiveQAPipeline(retriever=bm25_retriever, reader=reader)\n",
        "\n",
        "eval_result = pipe.eval(labels=labels_agg, params={})\n",
        "metrics = eval_result.calculate_metrics(simulated_top_k_reader=1)\n",
        "reader_eval[\"QA Pipeline (top-1)\"] = {k:v for k,v in metrics[\"Reader\"].items() for k in [\"exact_match\", \"f1\"]}"
      ],
      "metadata": {
        "id": "rAY5L1eUN9k7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_reader_eval({\"Reader\": reader_eval[\"Fine-tune on SQuAD + SubjQA\"], \n",
        "                  \"QA pipeline (top-1)\": reader_eval[\"QA Pipeline (top-1)\"]})"
      ],
      "metadata": {
        "id": "r7255U5pQjZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieval-Augmented Generation\n",
        "from haystack.nodes import RAGenerator\n",
        "from haystack.pipelines import GenerativeQAPipeline\n",
        "\n",
        "generator = RAGenerator(model_name_or_path=\"facebook/rag-token-nq\", embed_title=False, num_beams=5)\n",
        "pipe = GenerativeQAPipeline(generator=generator, retriever=dpr_retriever)\n",
        "\n",
        "def generate_answers(query, top_k_generator=3):\n",
        "  preds = pipe.run(query=query, \n",
        "                   params={\"Retriever\": {\"top_k\":5, \n",
        "                           \"filters\":{\"item_id\": [\"B0074BW614\"]}},\n",
        "                           \"Generator\": {\"top_k\": top_k_generator}})  \n",
        "  print(f\"Question: {preds['query']} \\n\")\n",
        "  for idx in range(top_k_generator):\n",
        "    print(f\"Answer {idx+1}: {preds['answers'][idx].answer}\")"
      ],
      "metadata": {
        "id": "v_uL7AOPVXp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_answers(query)\n",
        "generate_answers(\"What is the main drawback?\")"
      ],
      "metadata": {
        "id": "1X-QwfE4Wq7r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}