{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOr9Lw/Fb5cvrTmJEMR6cew",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsong-77/transformer-practice/blob/main/few-to-no-labels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7dO6e8aA9c4"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install nlpaug==1.1.7\n",
        "!pip install scikit-multilearn==0.2.0\n",
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset_url = \"https://git.io/nlp-with-transformers\"\n",
        "df_issues = pd.read_json(dataset_url, lines=True)\n",
        "df_issues.shape"
      ],
      "metadata": {
        "id": "Z7TdRIOxBlKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = [\"url\", \"id\", \"title\", \"user\", \"labels\", \"state\", \"created_at\", \"body\"]\n",
        "df_issues.loc[2, cols].to_frame()"
      ],
      "metadata": {
        "id": "DOK5q5aDCCA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_issues[\"labels\"] = df_issues[\"labels\"].apply(lambda x: [meta[\"name\"] for meta in x])\n",
        "df_issues[\"labels\"].head()"
      ],
      "metadata": {
        "id": "JSELfa3JCb5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_issues[\"labels\"].apply(lambda x: len(x)).value_counts().to_frame().T"
      ],
      "metadata": {
        "id": "8VybihQLDP_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_counts = df_issues[\"labels\"].explode().value_counts()\n",
        "print(f\"Number of labels: {len(df_counts)}\")\n",
        "df_counts.to_frame().head(8).T"
      ],
      "metadata": {
        "id": "QM4CNQ6UEDNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {\"Core: Tokenization\": \"tokenization\",\n",
        "             \"New model\": \"new model\",\n",
        "             \"Core: Modeling\": \"model training\",\n",
        "             \"Usage\": \"usage\",\n",
        "             \"Core: Pipeline\": \"pipeline\",\n",
        "             \"TensorFlow\": \"tensorflow or tf\",\n",
        "             \"PyTorch\": \"pytorch\",\n",
        "             \"Examples\": \"examples\",\n",
        "             \"Documentation\": \"documentation\"}\n",
        "\n",
        "def filter_labels(x):\n",
        "  return [label_map[label] for label in x if label in label_map]\n",
        "\n",
        "df_issues[\"labels\"] = df_issues[\"labels\"].apply(filter_labels)\n",
        "all_labels = list(label_map.values())"
      ],
      "metadata": {
        "id": "g1vnkfJXFvG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_counts = df_issues[\"labels\"].explode().value_counts()\n",
        "print(f\"Number of labels: {len(df_counts)}\")\n",
        "df_counts.to_frame().T"
      ],
      "metadata": {
        "id": "VdRjzK7sGW7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_issues[\"split\"] = \"unlabeled\"\n",
        "mask = df_issues[\"labels\"].apply(lambda x: len(x)) > 0\n",
        "df_issues.loc[mask, \"split\"] = \"labeled\"\n",
        "df_issues[\"split\"].value_counts().to_frame()"
      ],
      "metadata": {
        "id": "6emdBzXGHoFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in [\"title\", \"body\", \"labels\"]:\n",
        "  print(f\"{column}: {df_issues[column].iloc[26][:500]}\\n\")"
      ],
      "metadata": {
        "id": "PJEZDID4IS-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_issues[\"text\"] = df_issues.apply(lambda x: x[\"title\"] + \"\\n\\n\" + x[\"body\"], axis=1)\n",
        "\n",
        "len_before = len(df_issues)\n",
        "df_issues = df_issues.drop_duplicates(subset=\"text\")\n",
        "print(f\"Removed {(len_before-len(df_issues))/len_before:.2%} duplicates.\")"
      ],
      "metadata": {
        "id": "CCQ_AqesIxZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_issues[\"text\"].str.split().apply(len).hist(bins=np.linspace(0, 500, 50), grid=False, edgecolor=\"C0\")\n",
        "plt.title(\"Words per issue\")\n",
        "plt.xlabel(\"Number of words\")\n",
        "plt.ylabel(\"Number of issues\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yJNyysEGJKue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from skmultilearn.model_selection import iterative_train_test_split\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "mlb.fit([all_labels])\n",
        "\n",
        "def balanced_split(df, test_size=0.5):\n",
        "  labels = mlb.transform(df[\"labels\"])\n",
        "  ind = np.expand_dims(np.arange(len(df)), axis=1)\n",
        "  ind_train, _, ind_test, _ = iterative_train_test_split(ind, labels, test_size)\n",
        "\n",
        "  return df.iloc[ind_train[:, 0]], df.iloc[ind_test[:, 0]]"
      ],
      "metadata": {
        "id": "X8addlqCMZ9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "df_clean = df_issues[[\"text\", \"labels\", \"split\"]].reset_index(drop=True).copy()\n",
        "df_unsup = df_clean.loc[df_clean[\"split\"]==\"unlabeled\", [\"text\", \"labels\"]]\n",
        "df_sup = df_clean.loc[df_clean[\"split\"]==\"labeled\", [\"text\", \"labels\"]]\n",
        "\n",
        "np.random.seed(0)\n",
        "df_train, df_tmp = balanced_split(df_sup, test_size=0.5)\n",
        "df_valid, df_test = balanced_split(df_tmp, test_size=0.5)\n",
        "\n",
        "ds = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(df_train.reset_index(drop=True)),\n",
        "    \"valid\": Dataset.from_pandas(df_valid.reset_index(drop=True)),\n",
        "    \"test\": Dataset.from_pandas(df_test.reset_index(drop=True)),\n",
        "    \"unsup\": Dataset.from_pandas(df_unsup.reset_index(drop=True))})\n",
        "ds"
      ],
      "metadata": {
        "id": "iE5-dzaiRDsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "all_indices = np.expand_dims(list(range(len(ds[\"train\"]))), axis=1)\n",
        "indices_pool = all_indices\n",
        "labels = mlb.transform(ds[\"train\"][\"labels\"])\n",
        "train_samples = [8, 16, 32, 64, 128]\n",
        "train_slices, last_k = [], 0\n",
        "\n",
        "for i, k in enumerate(train_samples):\n",
        "  indices_pool, labels, new_slice, _ = iterative_train_test_split(indices_pool, labels, (k-last_k)/len(labels))\n",
        "  last_k = k\n",
        "  if i == 0: train_slices.append(new_slice)\n",
        "  else: train_slices.append(np.concatenate((train_slices[-1], new_slice)))\n",
        "\n",
        "train_slices.append(all_indices)\n",
        "train_samples.append(len(ds[\"train\"]))\n",
        "train_slices = [np.squeeze(train_slice) for train_slice in train_slices]"
      ],
      "metadata": {
        "id": "dgyKtvphS1Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Target split sizes:\")\n",
        "print(train_samples)\n",
        "print(\"Actual split sizes:\")\n",
        "print([len(x) for x in train_slices])"
      ],
      "metadata": {
        "id": "_-uhb0yxYJIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# naive bayes\n",
        "def prepare_labels(batch):\n",
        "  batch[\"label_ids\"] = mlb.transform(batch[\"labels\"])\n",
        "  return batch\n",
        "\n",
        "ds = ds.map(prepare_labels, batched=True)"
      ],
      "metadata": {
        "id": "8hK4UrQjYnXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "macro_scores, micro_scores = defaultdict(list), defaultdict(list)\n",
        "\n",
        "for train_slice in train_slices:\n",
        "  # get data\n",
        "  ds_train_sample = ds[\"train\"].select(train_slice)\n",
        "  y_train = np.array(ds_train_sample[\"label_ids\"])\n",
        "  y_test = np.array(ds[\"test\"][\"label_ids\"])\n",
        "  # encode\n",
        "  count_vect = CountVectorizer()\n",
        "  X_train_counts = count_vect.fit_transform(ds_train_sample[\"text\"])\n",
        "  X_test_counts = count_vect.transform(ds[\"test\"][\"text\"])\n",
        "  # train\n",
        "  classifier = BinaryRelevance(classifier=MultinomialNB())\n",
        "  classifier.fit(X_train_counts, y_train)\n",
        "  # predict and evaluate\n",
        "  y_pred_test = classifier.predict(X_test_counts)\n",
        "  clf_report = classification_report(y_test, y_pred_test, target_names=mlb.classes_, zero_division=0, output_dict=True)\n",
        "  # metrics\n",
        "  macro_scores[\"Naive Bayes\"].append(clf_report[\"macro avg\"][\"f1-score\"])\n",
        "  micro_scores[\"Naive Bayes\"].append(clf_report[\"micro avg\"][\"f1-score\"])"
      ],
      "metadata": {
        "id": "YoRpe3K-eSQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_metrics(micro_scores, macro_scores, sample_sizes, current_model):\n",
        "  fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
        "\n",
        "  for run in micro_scores.keys():\n",
        "    if run == current_model:\n",
        "      ax0.plot(sample_sizes, micro_scores[run], label=run, linewidth=2)\n",
        "      ax1.plot(sample_sizes, macro_scores[run], label=run, linewidth=2)\n",
        "    else:\n",
        "      ax0.plot(sample_sizes, micro_scores[run], label=run, linestyle=\"dashed\")\n",
        "      ax1.plot(sample_sizes, macro_scores[run], label=run, linestyle=\"dashed\")\n",
        "\n",
        "  ax0.set_title(\"Micro F1 scores\")\n",
        "  ax1.set_title(\"Macro F1 scores\")\n",
        "  ax0.set_ylabel(\"Test set F1 score\")\n",
        "  ax0.legend(loc=\"lower right\")\n",
        "  for ax in [ax0, ax1]:\n",
        "    ax.set_xlabel(\"Number of training samples\")\n",
        "    ax.set_xscale(\"log\")\n",
        "    ax.set_xticks(sample_sizes)\n",
        "    ax.set_xticklabels(sample_sizes)\n",
        "    ax.minorticks_off()\n",
        "  \n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "-UrwUPnshjLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(micro_scores, macro_scores, train_samples, \"Naive Bayes\")"
      ],
      "metadata": {
        "id": "w0tgLsBGiU3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# zero-shot\n",
        "from transformers import pipeline\n",
        "\n",
        "# prompt\n",
        "pipe = pipeline(\"fill-mask\", model=\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "EJlFaPTZinmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_desc = \"The main characters of the movie madacascar are a lion, a zebra, a giraffe, and a hippo. \"\n",
        "prompt = \"The movie is about [MASK].\"\n",
        "\n",
        "output = pipe(movie_desc + prompt, targets=[\"animals\", \"cars\"])\n",
        "for element in output:\n",
        "  print(f\"Token {element['token_str']}:\\t{element['score']:.3f}%\")"
      ],
      "metadata": {
        "id": "mpygwGDokG_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test entailment\n",
        "pipe = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "# pipe = pipeline(\"zero-shot-classification\", model=\"facebook/bart-base\")"
      ],
      "metadata": {
        "id": "3GqeoQgTlh0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zero_shot_pipeline(example):\n",
        "  output = pipe(example[\"text\"], all_labels, multi_label=True)\n",
        "  example[\"predicted_labels\"] = output[\"labels\"]\n",
        "  example[\"scores\"] = output[\"scores\"]\n",
        "  return example\n",
        "\n",
        "ds_zero_shot = ds[\"valid\"].map(zero_shot_pipeline)"
      ],
      "metadata": {
        "id": "F-z0P5ZYns1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_preds(example, threshold=None, topk=None):\n",
        "  preds = []\n",
        "  if threshold:\n",
        "    for label, score in zip(example[\"predicted_labels\"], example[\"scores\"]):\n",
        "      if score >= threshold:\n",
        "        preds.append(label)\n",
        "  elif topk:\n",
        "    for i in range(topk):\n",
        "      preds.append(example[\"predicted_labels\"][i])\n",
        "  else:\n",
        "    raise ValueError(\"Set either `threshold` or `topk`.\")\n",
        "\n",
        "  return {\"pred_label_ids\": list(np.squeeze(mlb.transform([preds])))}"
      ],
      "metadata": {
        "id": "2DMgELlS4n6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_clf_report(ds):\n",
        "  y_true = np.array(ds[\"label_ids\"])\n",
        "  y_pred = np.array(ds[\"pred_label_ids\"])\n",
        "  return classification_report(y_true, y_pred, target_names=mlb.classes_, zero_division=0, output_dict=True)"
      ],
      "metadata": {
        "id": "ok1kyH7Q5xk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "macros, micros = [], []\n",
        "topks = [1, 2, 3, 4]\n",
        "\n",
        "for topk in topks:\n",
        "  ds_zero_shot = ds_zero_shot.map(get_preds, batched=False, fn_kwargs={\"topk\": topk})\n",
        "  clf_report = get_clf_report(ds_zero_shot)\n",
        "  micros.append(clf_report['micro avg']['f1-score'])\n",
        "  macros.append(clf_report['macro avg']['f1-score'])"
      ],
      "metadata": {
        "id": "4wHOtWIF6kFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(topks, micros, label='Micro F1')\n",
        "plt.plot(topks, macros, label='Macro F1')\n",
        "plt.xlabel(\"Top-k\")\n",
        "plt.ylabel(\"F1-score\")\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bSgFmirD7u6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "macros, micros = [], []\n",
        "thresholds = np.linspace(0.01, 1, 100)\n",
        "\n",
        "for threshold in thresholds:\n",
        "  ds_zero_shot = ds_zero_shot.map(get_preds, fn_kwargs={\"threshold\": threshold})\n",
        "  clf_report = get_clf_report(ds_zero_shot)\n",
        "  micros.append(clf_report[\"micro avg\"][\"f1-score\"])\n",
        "  macros.append(clf_report[\"macro avg\"][\"f1-score\"])"
      ],
      "metadata": {
        "id": "8wH0LKA28Egd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(thresholds, micros, label=\"Micro F1\")\n",
        "plt.plot(thresholds, macros, label=\"Macro F1\")\n",
        "plt.xlabel(\"Threshold\")\n",
        "plt.ylabel(\"F1-score\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7meGeW7B8UEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_zero_shot = ds[\"test\"].map(zero_shot_pipeline)\n",
        "ds_zero_shot = ds_zero_shot.map(get_preds, fn_kwargs={\"topk\": 1})\n",
        "clf_report = get_clf_report(ds_zero_shot)\n",
        "\n",
        "for train_slice in train_slices:\n",
        "  macro_scores['Zero Shot'].append(clf_report['macro avg']['f1-score'])\n",
        "  micro_scores['Zero Shot'].append(clf_report['micro avg']['f1-score'])\n",
        "\n",
        "plot_metrics(micro_scores, macro_scores, train_samples, \"Zero Shot\")"
      ],
      "metadata": {
        "id": "PwJG3Xhn9NHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# few-shot\n",
        "# data augmentation\n",
        "from transformers import set_seed\n",
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "set_seed(3)\n",
        "aug = naw.ContextualWordEmbsAug(model_path=\"distilbert-base-uncased\", device=\"cpu\", action=\"substitute\")"
      ],
      "metadata": {
        "id": "LD73QfK7_SmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Transformers are the most popular toys\"\n",
        "print(f\"Original text: {text}\")\n",
        "print(f\"Augmented text: {aug.augment(text)}\")"
      ],
      "metadata": {
        "id": "5N1vwvDRBuv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_text(batch, transformations_per_example=1):\n",
        "  text_aug, label_ids = [], []\n",
        "  for text, labels in zip(batch[\"text\"], batch[\"label_ids\"]):\n",
        "    text_aug += [text]\n",
        "    label_ids += [labels]\n",
        "    for _ in range(transformations_per_example):\n",
        "      text_aug += [aug.augment(text)]\n",
        "      label_ids += [labels]\n",
        "\n",
        "  return {\"text\": text_aug, \"label_ids\": label_ids}\n",
        "\n",
        "ds_train_sample = ds_train_sample.map(augment_text, batched=True, remove_columns=ds_train_sample.column_names).shuffle(seed=42)"
      ],
      "metadata": {
        "id": "S1ta57qOBzl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "model_ckpt = \"miguelvictor/python-gpt2-medium\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "model = AutoModel.from_pretrained(model_ckpt)\n",
        "\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "  token_embeddings = model_output[0]\n",
        "  input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "\n",
        "  sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "  sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "  return sum_embeddings / sum_mask\n",
        "\n",
        "def embed_text(examples):\n",
        "  inputs = tokenizer(examples[\"text\"], padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "  with torch.no_grad():\n",
        "    model_output = model(**inputs)\n",
        "  pooled_embeds = mean_pooling(model_output, inputs[\"attention_mask\"])\n",
        "\n",
        "  return {\"embedding\": pooled_embeds.cpu().numpy()}"
      ],
      "metadata": {
        "id": "SqiB02h9EYsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "embs_train = ds[\"train\"].map(embed_text, batched=True, batch_size=16)\n",
        "embs_valid = ds[\"valid\"].map(embed_text, batched=True, batch_size=16)\n",
        "embs_test = ds[\"test\"].map(embed_text, batched=True, batch_size=16)"
      ],
      "metadata": {
        "id": "iY4S1mNWQV_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# k nearest neighbors\n",
        "import faiss\n",
        "\n",
        "embs_train.add_faiss_index(\"embedding\")\n",
        "\n",
        "def get_sample_preds(sample, m):\n",
        "  return (np.sum(sample[\"label_ids\"], axis=0) >= m).astype(int)\n",
        "\n",
        "def find_best_k_m(ds_train, valid_queries, valid_labels, max_k=17):\n",
        "  max_k = min(len(ds_train), max_k)\n",
        "  perf_micro = np.zeros((max_k, max_k))\n",
        "  perf_macro = np.zeros((max_k, max_k))\n",
        "\n",
        "  for k in range(1, max_k):\n",
        "    for m in range(1, k + 1):\n",
        "      _, samples = ds_train.get_nearest_examples_batch(\"embedding\", valid_queries, k=k)\n",
        "      y_pred = np.array([get_sample_preds(s, m) for s in samples])\n",
        "      clf_report = classification_report(valid_labels, y_pred, target_names=mlb.classes_, zero_division=0, output_dict=True)\n",
        "      perf_micro[k, m] = clf_report[\"micro avg\"][\"f1-score\"]\n",
        "      perf_macro[k, m] = clf_report[\"macro avg\"][\"f1-score\"]\n",
        "  return perf_micro, perf_macro"
      ],
      "metadata": {
        "id": "j6SFECmrS7K7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_labels = np.array(embs_valid[\"label_ids\"])\n",
        "valid_queries = np.array(embs_valid[\"embedding\"], dtype=np.float32)\n",
        "perf_micro, perf_macro = find_best_k_m(embs_train, valid_queries, valid_labels)"
      ],
      "metadata": {
        "id": "3t2eREYrdYnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True)\n",
        "ax0.imshow(perf_micro)\n",
        "ax1.imshow(perf_macro)\n",
        "\n",
        "ax0.set_title(\"micro scores\")\n",
        "ax0.set_ylabel(\"k\")\n",
        "ax1.set_title(\"macro scores\")\n",
        "for ax in [ax0, ax1]:\n",
        "  ax.set_xlim([0.5, 17 - 0.5])\n",
        "  ax.set_ylim([17 - 0.5, 0.5])\n",
        "  ax.set_xlabel(\"m\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-dHel-mAd9RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embs_train.drop_index(\"embedding\")\n",
        "test_labels = np.array(embs_test[\"label_ids\"])\n",
        "test_queries = np.array(embs_test[\"embedding\"], dtype=np.float32)\n",
        "\n",
        "for train_slice in train_slices:\n",
        "  # create faiss\n",
        "  embs_train_tmp = embs_train.select(train_slice)\n",
        "  embs_train_tmp.add_faiss_index(\"embedding\")\n",
        "  # find best k, m\n",
        "  perf_micro, _ = find_best_k_m(embs_train_tmp, valid_queries, valid_labels)\n",
        "  k, m = np.unravel_index(perf_micro.argmax(), perf_micro.shape)\n",
        "  # predictions\n",
        "  _, samples = embs_train_tmp.get_nearest_examples_batch(\"embedding\", test_queries, k=int(k))\n",
        "  y_pred = np.array([get_sample_preds(s, m) for s in samples])\n",
        "  # evaluate predictions\n",
        "  clf_report = classification_report(test_labels, y_pred, target_names=mlb.classes_, zero_division=0, output_dict=True)\n",
        "  macro_scores[\"Embedding\"].append(clf_report[\"macro avg\"][\"f1-score\"])\n",
        "  micro_scores[\"Embedding\"].append(clf_report[\"micro avg\"][\"f1-score\"])"
      ],
      "metadata": {
        "id": "vOyQ_VPjevsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(micro_scores, macro_scores, train_samples, \"Embedding\")"
      ],
      "metadata": {
        "id": "iAd6Etn9iNS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find-tuning\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
        "\n",
        "ckpt = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(ckpt)\n",
        "\n",
        "def tokenize(batch):\n",
        "  return tokenizer(batch[\"text\"], truncation=True, max_length=128)\n",
        "\n",
        "ds_enc = ds.map(tokenize, batched=True)\n",
        "ds_enc = ds_enc.remove_columns(['labels', 'text'])"
      ],
      "metadata": {
        "id": "KMqMSYf6lCVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_enc.set_format(\"torch\")\n",
        "ds_enc = ds_enc.map(lambda x: {\"label_ids_f\": x[\"label_ids\"].to(torch.float)},\n",
        "                    remove_columns=[\"label_ids\"])\n",
        "ds_enc = ds_enc.rename_column(\"label_ids_f\", \"label_ids\")"
      ],
      "metadata": {
        "id": "q9qzOQ1umH1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args_fine_tune = TrainingArguments(output_dir=\"./result\", num_train_epochs=20, learning_rate=3e-5,\n",
        "                                            lr_scheduler_type=\"constant\",\n",
        "                                            per_device_train_batch_size=4, per_device_eval_batch_size=32,\n",
        "                                            weight_decay=0.0,\n",
        "                                            evaluation_strategy=\"epoch\", save_strategy=\"epoch\",logging_strategy=\"epoch\",\n",
        "                                            load_best_model_at_end=True, metric_for_best_model=\"micro f1\",\n",
        "                                            save_total_limit=1, log_level=\"error\")"
      ],
      "metadata": {
        "id": "fnJy_geg2PjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import expit as sigmoid\n",
        "\n",
        "def compute_metrics(pred):\n",
        "  y_true = pred.label_ids\n",
        "  y_pred = sigmoid(pred.predictions)\n",
        "  y_pred = (y_pred > 0.5).astype(float)\n",
        "\n",
        "  clf_dict = classification_report(y_true, y_pred, target_names=all_labels, zero_division=0, output_dict=True)\n",
        "  return {\"micro f1\": clf_dict[\"micro avg\"][\"f1-score\"],\n",
        "          \"macro f1\": clf_dict[\"macro avg\"][\"f1-score\"]}"
      ],
      "metadata": {
        "id": "w6AnpZRE5xMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoConfig.from_pretrained(ckpt)\n",
        "config.num_labels = len(all_labels)\n",
        "config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "for train_slice in train_slices:\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(ckpt, config=config)\n",
        "\n",
        "  trainer = Trainer(model=model, tokenizer=tokenizer,\n",
        "                    args=training_args_fine_tune,\n",
        "                    compute_metrics=compute_metrics,\n",
        "                    train_dataset=ds_enc[\"train\"].select(train_slice),\n",
        "                    eval_dataset=ds_enc[\"valid\"])\n",
        "  \n",
        "  trainer.train()\n",
        "  pred = trainer.predict(ds_enc[\"test\"])\n",
        "  metrics = compute_metrics(pred)\n",
        "  macro_scores[\"Fine-tune (vanilla)\"].append(metrics[\"macro f1\"])\n",
        "  micro_scores[\"Fine-tune (vanilla)\"].append(metrics[\"micro f1\"])"
      ],
      "metadata": {
        "id": "K5Vw9oP76vnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(micro_scores, macro_scores, train_samples, \"Fine-tune (vanilla)\")"
      ],
      "metadata": {
        "id": "0OnqvPEo8LwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(batch):\n",
        "  return tokenizer(batch[\"text\"], truncation=True, max_length=128, return_special_tokens_mask=True)\n",
        "\n",
        "ds_mlm = ds.map(tokenize, batched=True)\n",
        "ds_mlm = ds_mlm.remove_columns([\"labels\", \"text\", \"label_ids\"])\n",
        "ds_mlm"
      ],
      "metadata": {
        "id": "ngkCKWMwgc8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling, set_seed\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
      ],
      "metadata": {
        "id": "gCQMr0YzjRHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(3)\n",
        "data_collator.return_tensors = \"np\"\n",
        "inputs = tokenizer(\"Transformers are awesome!\", return_tensors=\"np\")\n",
        "outputs = data_collator([{\"input_ids\": inputs[\"input_ids\"][0]}])\n",
        "\n",
        "original_input_ids = inputs[\"input_ids\"][0]\n",
        "masked_input_ids = outputs[\"input_ids\"][0]\n",
        "\n",
        "pd.DataFrame({\"Original tokens\": tokenizer.convert_ids_to_tokens(original_input_ids),\n",
        "              \"Masked tokens\": tokenizer.convert_ids_to_tokens(masked_input_ids),\n",
        "              \"Original input_ids\": original_input_ids,\n",
        "              \"Masked input_ids\": masked_input_ids,\n",
        "              \"Labels\": outputs[\"labels\"][0]}).T"
      ],
      "metadata": {
        "id": "KsMUSQn7jpIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator.return_tensors = \"pt\""
      ],
      "metadata": {
        "id": "jLkyZ13Ymlcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForMaskedLM\n",
        "\n",
        "training_args = TrainingArguments(output_dir=f\"{ckpt}-issues-128\", per_device_train_batch_size=32,\n",
        "                                  logging_strategy=\"epoch\", evaluation_strategy=\"epoch\", save_strategy=\"no\",\n",
        "                                  num_train_epochs=16, push_to_hub=False, log_level=\"error\", report_to=\"none\")\n",
        "\n",
        "model = AutoModelForMaskedLM.from_pretrained(ckpt)\n",
        "trainer = Trainer(model=model, tokenizer=tokenizer,\n",
        "                  args=training_args, data_collator=data_collator,\n",
        "                  train_dataset=ds_mlm[\"unsup\"], eval_dataset=ds_mlm[\"train\"])\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "m8Ywb3q-nNb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_log = pd.DataFrame(trainer.state.log_history)\n",
        "\n",
        "df_log.dropna(subset=[\"eval_loss\"]).reset_index()[\"eval_loss\"].plot(label=\"Validation\")\n",
        "df_log.dropna(subset=[\"loss\"]).reset_index()[\"loss\"].plot(label=\"Train\")\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FT23brUNpJqQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}